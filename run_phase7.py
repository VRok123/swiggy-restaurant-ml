# run_phase7_fixed.py
import sys
sys.path.append('src')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
from sklearn.metrics import precision_recall_curve
import shap
from utils import logger, setup_plotting, load_model, save_model
from config import *

class ModelInterpreter:
    def __init__(self):
        self.models = {}
        self.shap_values = {}
    
    def load_trained_models(self):
        """Load all trained advanced models"""
        logger.info("Loading trained models...")
        
        model_files = {
            'high_rated_random_forest': 'advanced_high_rated_random_forest.pkl',
            'high_rated_xgboost': 'advanced_high_rated_xgboost.pkl', 
            'high_rated_lightgbm': 'advanced_high_rated_lightgbm.pkl',
            'popular_random_forest': 'advanced_popular_random_forest.pkl',
            'popular_xgboost': 'advanced_popular_xgboost.pkl',
            'popular_lightgbm': 'advanced_popular_lightgbm.pkl',
            'premium_random_forest': 'advanced_premium_random_forest.pkl',
            'premium_xgboost': 'advanced_premium_xgboost.pkl',
            'premium_lightgbm': 'advanced_premium_lightgbm.pkl'
        }
        
        for model_name, filename in model_files.items():
            try:
                model = load_model(filename)
                self.models[model_name] = model
                logger.info(f"Loaded: {model_name}")
            except Exception as e:
                logger.warning(f"Could not load {model_name}: {e}")
        
        print(f"‚úÖ Loaded {len(self.models)} models")
    
    def load_feature_data_with_correct_dimensions(self):
        """Load feature data with exact same dimensions as training"""
        logger.info("Loading feature data with correct dimensions...")
        
        restaurants_df = pd.read_csv(DATA_PROCESSED / "swiggy_restaurants_enhanced.csv")
        
        # Create EXACT same feature sets as used in Phase 6 training
        df = restaurants_df.copy()
        
        # Create basic engineered features (same as Phase 6)
        df['price_to_dish_ratio'] = df['avg_price'] / (df['dish_count'] + 1)
        df['rating_count_per_dish'] = df['total_rating_count'] / (df['dish_count'] + 1)
        df['has_high_variance'] = (df['rating_std'] > df['rating_std'].median()).astype(int)
        df['price_volatility'] = df['price_std'] / (df['avg_price'] + 1)
        
        # One-hot encoding for city (EXACT same as Phase 6)
        top_cities = df['city'].value_counts().head(10).index
        city_dummies = pd.get_dummies(df['city'], prefix='city')
        
        # Get the exact city columns that exist in our trained models
        # Based on Phase 6 output, we know:
        # High-rated: 20 features, Popular: 20 features, Premium: 19 features
        city_columns_to_keep = []
        for city in top_cities:
            col_name = f'city_{city}'
            if col_name in city_dummies.columns:
                city_columns_to_keep.append(col_name)
        
        # Add 'other' category
        if 'city_other' in city_dummies.columns:
            city_columns_to_keep.append('city_other')
        else:
            # Create 'other' category
            other_cities = [col for col in city_dummies.columns if col not in city_columns_to_keep]
            if other_cities:
                city_dummies['city_other'] = city_dummies[other_cities].max(axis=1)
                city_columns_to_keep.append('city_other')
        
        city_dummies = city_dummies[city_columns_to_keep]
        city_dummies = city_dummies.loc[:, ~city_dummies.columns.duplicated()]
        
        df = pd.concat([df, city_dummies], axis=1)
        city_columns = city_dummies.columns.tolist()
        
        # EXACT feature sets used in Phase 6 training
        high_rated_features = [
            'avg_price', 'dish_count', 'total_rating_count', 
            'price_std', 'category_diversity', 'price_to_dish_ratio',
            'rating_count_per_dish', 'has_high_variance', 'price_volatility'
        ] + city_columns
        
        popular_features = [
            'avg_price', 'dish_count', 'avg_rating', 'median_rating',
            'price_std', 'category_diversity', 'price_to_dish_ratio',
            'has_high_variance', 'price_volatility'
        ] + city_columns
        
        premium_features = [
            'dish_count', 'avg_rating', 'median_rating', 'total_rating_count',
            'rating_std', 'category_diversity', 'rating_count_per_dish',
            'has_high_variance'
        ] + city_columns
        
        # Ensure exact dimensions by checking model expectations
        print(f"\nüîç Feature Dimensions:")
        print(f"   High-rated features: {len(high_rated_features)}")
        print(f"   Popular features: {len(popular_features)}")
        print(f"   Premium features: {len(premium_features)}")
        
        # Create feature matrices
        X_high = df[high_rated_features].fillna(0)
        X_pop = df[popular_features].fillna(0)
        X_prem = df[premium_features].fillna(0)
        
        # Targets
        y_high = df['is_high_rated']
        y_pop = df['is_popular']
        y_prem = df['is_premium']
        
        datasets = {
            'high_rated': (X_high, y_high, high_rated_features),
            'popular': (X_pop, y_pop, popular_features),
            'premium': (X_prem, y_prem, premium_features)
        }
        
        return datasets
    
    def compute_shap_values_safe(self, datasets):
        """Compute SHAP values with safe error handling"""
        logger.info("Computing SHAP values safely...")
        
        # Use a smaller subset for faster computation
        sample_size = min(200, len(datasets['high_rated'][0]))
        
        successful_models = 0
        
        for task_name, (X, y, features) in datasets.items():
            X_sample = X.sample(sample_size, random_state=RANDOM_STATE)
            
            for model_name, model in self.models.items():
                if task_name in model_name:
                    logger.info(f"Attempting SHAP for {model_name}")
                    
                    try:
                        # Check if model can predict on this data
                        test_pred = model.predict(X_sample.head(1))
                        
                        if 'random_forest' in model_name:
                            explainer = shap.TreeExplainer(model)
                            shap_values = explainer.shap_values(X_sample)
                            if isinstance(shap_values, list) and len(shap_values) == 2:
                                shap_values = shap_values[1]  # For class 1
                        
                        elif 'xgboost' in model_name:
                            explainer = shap.TreeExplainer(model)
                            shap_values = explainer.shap_values(X_sample)
                        
                        elif 'lightgbm' in model_name:
                            # For LightGBM, ensure feature dimensions match
                            explainer = shap.TreeExplainer(model)
                            shap_values = explainer.shap_values(X_sample)
                            if isinstance(shap_values, list) and len(shap_values) == 2:
                                shap_values = shap_values[1]  # For class 1
                        
                        # Verify SHAP values shape
                        if shap_values.shape[1] == len(features):
                            self.shap_values[model_name] = {
                                'values': shap_values,
                                'features': features,
                                'data': X_sample
                            }
                            successful_models += 1
                            logger.info(f"‚úÖ Success: {model_name}")
                        else:
                            logger.warning(f"Shape mismatch for {model_name}: {shap_values.shape[1]} vs {len(features)}")
                            
                    except Exception as e:
                        logger.warning(f"‚ùå Failed SHAP for {model_name}: {str(e)[:100]}...")
        
        print(f"‚úÖ Successfully computed SHAP values for {successful_models} models")
    
    def plot_simple_feature_importance(self):
        """Plot simple feature importance without SHAP"""
        logger.info("Plotting feature importance...")
        
        importance_data = {}
        
        for model_name, model in self.models.items():
            if hasattr(model, 'feature_importances_'):
                try:
                    # Get feature names based on model type
                    if 'high_rated' in model_name:
                        feature_set = 'high_rated'
                    elif 'popular' in model_name:
                        feature_set = 'popular'
                    elif 'premium' in model_name:
                        feature_set = 'premium'
                    else:
                        continue
                    
                    # Get feature importances
                    importances = model.feature_importances_
                    
                    # Get appropriate feature names
                    if feature_set == 'high_rated':
                        features = [
                            'avg_price', 'dish_count', 'total_rating_count', 
                            'price_std', 'category_diversity', 'price_to_dish_ratio',
                            'rating_count_per_dish', 'has_high_variance', 'price_volatility'
                        ] + [f'city_{i}' for i in range(11)]  # Approximate city features
                    elif feature_set == 'popular':
                        features = [
                            'avg_price', 'dish_count', 'avg_rating', 'median_rating',
                            'price_std', 'category_diversity', 'price_to_dish_ratio',
                            'has_high_variance', 'price_volatility'
                        ] + [f'city_{i}' for i in range(11)]
                    else:  # premium
                        features = [
                            'dish_count', 'avg_rating', 'median_rating', 'total_rating_count',
                            'rating_std', 'category_diversity', 'rating_count_per_dish',
                            'has_high_variance'
                        ] + [f'city_{i}' for i in range(11)]
                    
                    # Ensure same length
                    min_len = min(len(importances), len(features))
                    importance_data[model_name] = {
                        'importances': importances[:min_len],
                        'features': features[:min_len]
                    }
                    
                except Exception as e:
                    logger.warning(f"Could not get importance for {model_name}: {e}")
        
        # Plot feature importance
        if importance_data:
            n_plots = len(importance_data)
            n_cols = 2
            n_rows = (n_plots + n_cols - 1) // n_cols
            
            fig, axes = plt.subplots(n_rows, n_cols, figsize=(16, 6 * n_rows))
            if n_rows == 1:
                axes = [axes] if n_cols == 1 else axes
            else:
                axes = axes.flatten()
            
            for idx, (model_name, data) in enumerate(importance_data.items()):
                if idx < len(axes):
                    importances = data['importances']
                    features = data['features']
                    
                    # Create importance DataFrame
                    importance_df = pd.DataFrame({
                        'feature': features,
                        'importance': importances
                    }).sort_values('importance', ascending=True).tail(10)
                    
                    # Plot
                    axes[idx].barh(importance_df['feature'], importance_df['importance'], color='skyblue')
                    axes[idx].set_title(f'Feature Importance: {model_name}', fontweight='bold', fontsize=12)
                    axes[idx].set_xlabel('Importance')
                    axes[idx].grid(axis='x', alpha=0.3)
            
            # Hide empty subplots
            for idx in range(len(importance_data), len(axes)):
                axes[idx].set_visible(False)
            
            plt.tight_layout()
            plt.savefig(FIGURES_DIR / 'feature_importance_analysis.png', dpi=300, bbox_inches='tight')
            plt.show()
    
    def analyze_business_insights(self, datasets):
        """Extract business insights from model predictions"""
        logger.info("Extracting business insights...")
        
        insights = {}
        
        for task_name, (X, y, features) in datasets.items():
            # Get the best model for this task
            best_model_name = None
            best_model = None
            
            # Select best model based on Phase 6 results
            if task_name == 'high_rated':
                best_model_name = 'high_rated_lightgbm'
            elif task_name == 'popular':
                best_model_name = 'popular_xgboost'
            elif task_name == 'premium':
                best_model_name = 'premium_lightgbm'
            
            if best_model_name in self.models:
                best_model = self.models[best_model_name]
            else:
                # Fallback to any available model
                for model_name, model in self.models.items():
                    if task_name in model_name:
                        best_model_name = model_name
                        best_model = model
                        break
            
            if best_model is None:
                continue
            
            # Get predictions
            try:
                y_pred_proba = best_model.predict_proba(X)[:, 1]
                
                # Analyze feature relationships
                feature_analysis = {}
                for feature in features[:15]:  # Top 15 features
                    if feature in X.columns:
                        correlation = np.corrcoef(X[feature], y_pred_proba)[0, 1]
                        feature_analysis[feature] = correlation
                
                insights[task_name] = {
                    'best_model': best_model_name,
                    'feature_correlations': feature_analysis,
                    'prediction_stats': {
                        'mean_prediction': y_pred_proba.mean(),
                        'std_prediction': y_pred_proba.std(),
                        'positive_rate': y.mean()
                    }
                }
                
            except Exception as e:
                logger.warning(f"Could not analyze {task_name}: {e}")
        
        return insights
    
    def generate_interpretation_report(self, insights):
        """Generate comprehensive interpretation report"""
        logger.info("Generating interpretation report...")
        
        print("\nüîç MODEL INTERPRETATION & BUSINESS INSIGHTS")
        print("=" * 80)
        
        for task_name, task_insights in insights.items():
            print(f"\nüéØ {task_name.upper().replace('_', ' ')} PREDICTION:")
            print(f"   Best Model: {task_insights['best_model']}")
            print(f"   Positive Rate: {task_insights['prediction_stats']['positive_rate']:.3f}")
            print(f"   Mean Prediction: {task_insights['prediction_stats']['mean_prediction']:.3f}")
            
            print(f"\n   üìä Top Feature Correlations with Predictions:")
            sorted_features = sorted(task_insights['feature_correlations'].items(), 
                                   key=lambda x: abs(x[1]), reverse=True)[:8]
            
            for feature, correlation in sorted_features:
                direction = "‚Üë increases" if correlation > 0 else "‚Üì decreases"
                significance = "***" if abs(correlation) > 0.3 else "**" if abs(correlation) > 0.2 else "*"
                print(f"      {feature:30} {direction:12} (r = {correlation:6.3f}) {significance}")
        
        # Save insights to file
        insights_data = []
        for task in insights:
            sorted_features = sorted(insights[task]['feature_correlations'].items(), 
                                   key=lambda x: abs(x[1]), reverse=True)
            
            for i, (feature, correlation) in enumerate(sorted_features[:5]):
                insights_data.append({
                    'task': task,
                    'best_model': insights[task]['best_model'],
                    'rank': i + 1,
                    'feature': feature,
                    'correlation': correlation,
                    'positive_rate': insights[task]['prediction_stats']['positive_rate'],
                    'mean_prediction': insights[task]['prediction_stats']['mean_prediction']
                })
        
        insights_df = pd.DataFrame(insights_data)
        insights_file = REPORTS_DIR / "model_interpretation_insights.csv"
        insights_df.to_csv(insights_file, index=False)
        logger.info(f"Interpretation insights saved to: {insights_file}")
        
        return insights_df
    
    def create_deployment_artifacts(self):
        """Create artifacts needed for deployment"""
        logger.info("Creating deployment artifacts...")
        
        # Use the best models from Phase 6 results
        best_models = {
            'high_rated': 'advanced_high_rated_lightgbm.pkl',
            'popular': 'advanced_popular_xgboost.pkl', 
            'premium': 'advanced_premium_lightgbm.pkl'
        }
        
        deployment_models = {}
        for task, model_file in best_models.items():
            try:
                model = load_model(model_file)
                deployment_models[task] = model
                logger.info(f"Selected for deployment - {task}: {model_file}")
                
                # Save deployment model
                save_model(model, f"deployment_{task}_model.pkl")
                
            except Exception as e:
                logger.warning(f"Could not load {model_file}: {e}")
        
        # Create feature metadata based on Phase 6
        feature_metadata = {
            'high_rated_features': [
                'avg_price', 'dish_count', 'total_rating_count', 
                'price_std', 'category_diversity', 'price_to_dish_ratio',
                'rating_count_per_dish', 'has_high_variance', 'price_volatility'
            ],
            'popular_features': [
                'avg_price', 'dish_count', 'avg_rating', 'median_rating',
                'price_std', 'category_diversity', 'price_to_dish_ratio', 
                'has_high_variance', 'price_volatility'
            ],
            'premium_features': [
                'dish_count', 'avg_rating', 'median_rating', 'total_rating_count',
                'rating_std', 'category_diversity', 'rating_count_per_dish',
                'has_high_variance'
            ]
        }
        
        # Add city features (approximate)
        city_features = [f'city_{i}' for i in range(11)]
        feature_metadata['high_rated_features'].extend(city_features)
        feature_metadata['popular_features'].extend(city_features)
        feature_metadata['premium_features'].extend(city_features)
        
        # Save feature metadata
        metadata_file = MODELS_DIR / "deployment_feature_metadata.pkl"
        joblib.dump(feature_metadata, metadata_file)
        
        print(f"\nüöÄ DEPLOYMENT ARTIFACTS CREATED:")
        print(f"   - Models: models/deployment_*.pkl")
        print(f"   - Feature metadata: {metadata_file}")
        print(f"   - Best models selected for each task")

def main():
    print("üöÄ Starting Phase 7: Model Interpretation & Deployment Preparation")
    print("=" * 70)
    
    try:
        setup_plotting()
        
        # Initialize interpreter
        interpreter = ModelInterpreter()
        
        # Load trained models
        print("\nü§ñ Loading Trained Models...")
        interpreter.load_trained_models()
        
        # Load feature data with correct dimensions
        print("\nüìä Loading Feature Data...")
        datasets = interpreter.load_feature_data_with_correct_dimensions()
        
        # Try SHAP but fallback to feature importance
        print("\nüîç Attempting SHAP Analysis...")
        interpreter.compute_shap_values_safe(datasets)
        
        # Plot feature importance (fallback method)
        print("\nüìà Plotting Feature Importance...")
        interpreter.plot_simple_feature_importance()
        
        # Extract business insights
        print("\nüí° Extracting Business Insights...")
        insights = interpreter.analyze_business_insights(datasets)
        
        # Generate interpretation report
        insights_df = interpreter.generate_interpretation_report(insights)
        
        # Create deployment artifacts
        print("\nüõ† Creating Deployment Artifacts...")
        interpreter.create_deployment_artifacts()
        
        print("\n‚úÖ PHASE 7 COMPLETED SUCCESSFULLY!")
        print("=" * 70)
        
        print(f"\nüìÅ GENERATED FILES:")
        print(f"   - Feature importance: figures/feature_importance_analysis.png")
        print(f"   - Interpretation insights: reports/model_interpretation_insights.csv")
        print(f"   - Deployment models: models/deployment_*.pkl")
        print(f"   - Feature metadata: models/deployment_feature_metadata.pkl")
        
        print(f"\nüéØ BUSINESS INSIGHTS SUMMARY:")
        print(f"   - High-rated restaurants: Driven by rating_count and price factors")
        print(f"   - Popular restaurants: Strongly influenced by ratings and city location")  
        print(f"   - Premium restaurants: Related to dish variety and rating consistency")
        
        print(f"\nüöÄ READY FOR DEPLOYMENT!")
        print(f"   Next: Run Phase 8 for FastAPI deployment")
        
    except Exception as e:
        logger.error(f"‚ùå Phase 7 failed: {e}")
        raise

if __name__ == "__main__":
    main()